{"cells":[{"cell_type":"markdown","metadata":{"id":"K93akE52tBCh"},"source":["#Download Required Dependecies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90621,"status":"ok","timestamp":1697223738731,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"309D2N81C29K","outputId":"48c5594b-95a3-49da-dddd-053558829f4f"},"outputs":[],"source":["!pip install 'git+https://github.com/facebookresearch/fvcore'\n","!pip install simplejson\n","!conda install av -c conda-forge\n","!pip install av\n","!pip install -U iopath\n","!pip install psutil\n","!pip install opencv-python\n","\n","## Install package for Ray Tuning\n","!pip install \"ray[tune]\"\n","!pip install optuna\n","## Done for Ray Tuning\n","\n","# !pip install decord\n","!pip install timm\n","## Install rarfile to extract .rar file\n","!pip install rarfile\n","## Install torchmetrics to evaluate model\n","!pip install torchmetrics\n","## Clone and develop\n","!git clone https://github.com/Sense-X/UniFormer\n","%cd UniFormer/video_classification\n","!python setup.py build develop"]},{"cell_type":"markdown","metadata":{"id":"xnrriyDYtc8u"},"source":["# Create directory for datasets"]},{"cell_type":"markdown","metadata":{"id":"LinSCUSnv_gn"},"source":["\n","##Download and extract test datasets/ Download pretrained model"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":200748,"status":"ok","timestamp":1697223970749,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"zsm0dir_RMo_","outputId":"acca113a-8398-4645-fa18-876e0cf9c6cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["video directory exists.\n","Google Drive is not mounted.\n","Mounted at /content/drive\n","current_directory: /content/UniFormer/video_classification\n","Did not find 'pretrained_model' directory, creating one...\n"]}],"source":["from pathlib import Path\n","from os import path, makedirs, listdir, getcwd\n","from google.colab import drive\n","import rarfile\n","from shutil import copyfile\n","\n","def create_dir (dir):\n","    data_path = Path (dir)\n","\n","    if data_path.is_dir ():\n","        print(f\"{data_path} directory exists.\")\n","    else:\n","        print(f\"Did not find '{data_path}' directory, creating one...\")\n","        data_path.mkdir(parents=True, exist_ok=True)\n","\n","## Create 'video' folder, then extract UCF50 to video\n","data_path = 'video'\n","create_dir (data_path)\n","\n","root = '/content/drive'\n","\n","drive_is_mounted = path.exists(root)\n","if drive_is_mounted:\n","    print(\"Google Drive is already mounted.\")\n","else:\n","    print(\"Google Drive is not mounted.\")\n","    drive.mount(root)\n","\n","EXISTING_PATH = root + '/MyDrive/Colab Notebooks/USF_VideoUnderstanding'\n","EXTRACTION_PATH = data_path\n","zipped_filename = '/UCF50.rar'\n","\n","num_folders_to_extract = 5\n","num_files_per_folder = 10\n","with rarfile.RarFile (EXISTING_PATH + zipped_filename, 'r') as f:\n","    # # Get a list of files and folders in the RAR archive\n","    # archive_items = f.infolist()\n","    # print (archive_items)\n","    # # Create a dictionary to track the number of files extracted from each folder\n","    # folder_file_counts = {}\n","    # # Create a counter to keep track of the extracted folders\n","    # extracted_folders = 0\n","    # # Iterate over the items in the archive\n","    # for item in archive_items:\n","    #     # Check if the item is not a folder\n","    #     if not item.is_dir():\n","    #         folder = EXTRACTION_PATH / Path (item.filename).parent\n","    #         if not folder.is_dir ():\n","    #             extracted_folders += 1\n","    #         if extracted_folders > num_folders_to_extract:\n","    #             break\n","    #         # Check if the folder has reached the desired number of files\n","    #         folder_path = str(folder)\n","    #         if folder_path not in folder_file_counts:\n","    #             folder_file_counts[folder_path] = 1\n","    #         else:\n","    #             folder_file_counts[folder_path] += 1\n","    #         if folder_file_counts[folder_path] > num_files_per_folder:\n","    #             continue  # Skip this file if the folder already has enough\n","    #         f.extract(item, path=EXTRACTION_PATH)\n","    # Comment everything and uncomment following code to extract every folder\n","    f.extractall (EXTRACTION_PATH)\n","\n","## Upload the pretrained model from google drive\n","current_directory = getcwd ()\n","print (f'current_directory: {current_directory}')\n","\n","data_path = 'pretrained_model'\n","create_dir (data_path)\n","\n","pretrained_model_path = EXISTING_PATH + '/PretrainedModel'\n","model_name = '/uniformer_small_k400_16x8.pth'\n","copyfile (pretrained_model_path + model_name, data_path + model_name)\n","# !wget -O pretrained_model/model.pth \"https://drive.google.com/file/d/1-c835NJjg_015WxLBQF-1cPdQsyWMWfY/view?usp=sharing\"\n","drive.flush_and_unmount()"]},{"cell_type":"markdown","metadata":{"id":"t7_06w-zyYAY"},"source":["## Create name-label .csv file from Test Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":274},"executionInfo":{"elapsed":245,"status":"error","timestamp":1694616681747,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"Nz22q9Y0Tifu","outputId":"8ef6ba57-c012-4392-cf80-f54b2c52b01c"},"outputs":[],"source":["import zipfile\n","from os import scandir, path, walk, listdir\n","import csv\n","import torch\n","\n","def find_classes (directory):\n","    class_names = [item.name for item in scandir (directory) if item.is_dir()]\n","    if not class_names:\n","        raise FileNotFoundError (f\"Couldn't find any classes in {directory}\")\n","    class_to_idx = {class_name: i for i, class_name in enumerate (class_names)}\n","    return class_names, class_to_idx\n","\n","model_path = path.join('pretrained_model', 'uniformer_small_k400_16x8.pth')\n","# with zipfile.ZipFile(model_path, 'r') as zip_ref:\n","#     zip_ref.extractall('/content/pretrained_model')\n","\n","\n","state_dict = torch.load (model_path, map_location='cpu')\n","\n","label_file = './data_list/k400/kinetics_400_categroies.txt'\n","category = []\n","for x in open(label_file):\n","    category.append(x.rstrip().split('\\t')[0])\n","\n","num_class = len(category)\n","\n","test_folder = zipped_filename.rsplit('/', 1)[-1].rsplit('.', 1)[0]\n","main_folder_path = './video/' + test_folder\n","class_names, class_to_idx = find_classes (main_folder_path)\n","\n","def create_file_csv (src_dir, targ_dir, name, file_type: str):\n","    csv_file_path = targ_dir + name + '.csv'\n","    with open(csv_file_path, 'w', newline='') as csv_file:\n","        csv_writer = csv.writer(csv_file)\n","\n","        # Traverse through subdirectories and files\n","        for root, dirs, files in walk(src_dir):\n","            for folder_name in dirs:\n","                folder_path = path.join(root, folder_name)\n","                for video_name in listdir(folder_path):\n","                    if video_name.endswith(file_type):  # Adjust the extension if needed\n","                        csv_writer.writerow([video_name, class_to_idx[folder_name]])\n","\n","create_file_csv (main_folder_path, './video/', test_folder, '.avi')\n","print (f\"{label_file}'s category: {category}\")"]},{"cell_type":"markdown","metadata":{"id":"48e5WO_rzOZT"},"source":["# Load pretrained model and make preds on the test datasets"]},{"cell_type":"markdown","metadata":{"id":"pFg7WztzEZ02"},"source":["## Modules to be used"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5827,"status":"ok","timestamp":1697223976565,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"GT6grlc2RenE","outputId":"a5222d73-b67f-4b72-9b03-d0d0c239d34e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n","/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n","  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"]}],"source":["from vis.model import uniformer_small\n","\n","from collections import OrderedDict\n","\n","import os\n","import cv2\n","import torchvision\n","from torch import nn, optim\n","import torch.nn.functional as F\n","import argparse\n","import numpy as np\n","from PIL import Image # module to read image\n","from scipy.ndimage import zoom\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","# from decord import VideoReader # Read the video format into iterator of frames (images)\n","# from decord import cpu\n","from google.colab import drive\n","import sys\n","from torch.utils.data import Dataset, random_split, DataLoader\n","from pathlib import Path\n","from torch.optim import lr_scheduler\n","import itertools\n","from math import floor\n","from torchvision.io import VideoReader as vr\n","from torchvision.io import read_video\n","import torchvision.transforms.v2 as t\n","\n","import random\n","import pickle\n","\n","import zipfile\n","from os import scandir, path, walk, listdir\n","import csv\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"OvqRMxzCVI0r"},"source":["## Transform the the video and Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HAfHMkXNn9I4"},"outputs":[],"source":["from vis.transforms import (GroupNormalize, GroupScale, GroupCenterCrop, Stack, ToTorchFormatTensor)\n","\n","def get_index(num_frames, num_segments, dense_sample_rate = 8, method = 'dense'):\n","    if method == 'dense':\n","        sample_range = num_segments * dense_sample_rate\n","        sample_pos = max(1, 1 + num_frames - sample_range)\n","        t_stride = dense_sample_rate\n","        start_idx = 0 if sample_pos == 1 else sample_pos // 2\n","        offsets = np.array([\n","            (idx * t_stride + start_idx) %\n","            num_frames for idx in range(num_segments)\n","        ])\n","    else:\n","        if num_frames > num_segments:\n","            tick = num_frames / float(num_segments)\n","            offsets = np.array([int(tick / 2.0 + tick * x) for x in range(num_segments)])\n","        else:\n","            offsets = np.zeros((num_segments,))\n","    return offsets\n","\n","def load_images (video, selected_frames, transform1, transform2, crop_size):\n","    t_size = len (selected_frames)\n","    images = np.zeros ((t_size, crop_size, crop_size, 3))\n","    orig_imgs = np.zeros_like(images)\n","    images_group = list()\n","    for i, frame_index in enumerate (selected_frames):\n","        img = Image.fromarray (video[frame_index].asnumpy())\n","        images_group.append (img)\n","        r_image = np.array (img)[:,:,::-1] # Reverse the format from BGR to RGB\n","        orig_imgs[i] = transform2 ([Image.fromarray(r_image)])\n","    torch_imgs = transform1 (images_group)\n","    return np.expand_dims (orig_imgs, 0), torch_imgs # Use torch_imgs for prediction because it is converted to Tensor by transfrom1\n","\n","def get_img(index, path_prefix, train_list, crop_size, length=16):\n","    path, label = train_list[index]\n","    video_path = os.path.join(path_prefix, path)\n","    vr = VideoReader(video_path, ctx=cpu(0))\n","    num_frames = len(vr)\n","    frame_indices = get_index(num_frames, length, dense_sample_rate=32)\n","    RGB_vid, vid = load_images(vr, frame_indices, transform1, transform2, crop_size)\n","\n","    return RGB_vid, vid, path, int(label)\n","\n","train_list = list()\n","with open('./video/UCF50.csv', 'r') as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        line = line.rstrip()\n","        train_list.append(line.split(','))\n","    # \"'train_list = [(name_of_the_video, label_of_the_video)]'\"\n","\n","vid_name = list ()\n","for path, label in train_list:\n","    if int (label) == class_to_idx['BenchPress']:\n","        vid_name.append (path)\n","\n","path_prefix = './video/UCF50/BenchPress'\n","path = random.choice (vid_name)\n","# RGB_vid, vid, path, label = get_img (index, path_prefix)\n","\n","## 'Define trans\n","crop_size = 448\n","scale_size = 512\n","input_mean = [0.485, 0.456, 0.406]\n","input_std = [0.229, 0.224, 0.225]\n","\n","input_mean2 = [0.5, 0.5, 0.5]\n","input_std2 = [0.5, 0.5, 0.5]\n","\n","transform1 = torchvision.transforms.Compose([\n","    GroupScale(int(scale_size)),\n","    GroupCenterCrop(crop_size),\n","    Stack(),\n","    ToTorchFormatTensor(),\n","    GroupNormalize(input_mean, input_std),\n","])\n","\n","transform2 = torchvision.transforms.Compose([\n","    GroupScale(int(scale_size)),\n","    GroupCenterCrop(crop_size),\n","    Stack(),\n","])\n","\n","video_path = os.path.join(path_prefix, path)\n","device = 'cuda' if torch.cuda.is_available () else 'cpu'\n","vr = VideoReader (video_path, ctx=cpu(0))\n","length = 16\n","num_frames = len(vr)\n","frame_indices = get_index(num_frames, length, dense_sample_rate=32)\n","RGB_vid, vid = load_images (vr, frame_indices, transform1, transform2, crop_size)\n","\n","TC, H, W = vid.shape\n","print (f\"{video_path}'s shape: {vid.shape}\")\n","inputs = vid.reshape(1, TC//3, 3, H, W).permute(0, 2, 1, 3, 4).to (device)\n","print (f\"input's shape: {inputs.shape}\")\n","\n","## 'Load the model and predict\n","model = uniformer_small ().to(device)\n","model.load_state_dict (state_dict)\n","model.eval()\n","\n","def main ():\n","    with torch.no_grad():\n","        pred, y = model(inputs)\n","        pred = torch.argmax(pred).item()\n","    ## Predict end'\n","\n","    print(\"path: {}\".format(path))\n","    # print(\"ground truth: {}-{}\".format(label, category[label]))\n","    print(\"prediction: {}-{}\".format(pred, category[pred]))\n","\n","    plt.rcParams['savefig.dpi'] = 200\n","    plt.rcParams['figure.dpi'] = 200\n","    plt.figure(figsize=(16, 4))\n","    gs = gridspec.GridSpec(2,8,wspace=0.0, hspace=0.0)\n","\n","    for i in range(2):\n","        for j in range(8):\n","            plt.subplot(gs[i,j])\n","            temp = RGB_vid[0][i*8+j]\n","            plt.imshow(temp[:,:,::-1]/255.)\n","            plt.axis('off')\n","\n","    plt.show()\n","\n","    # for i in range(16):\n","    #     temp = RGB_vid[0][i]\n","    #     plt.imshow(temp[:,:,::-1]/255.)\n","    #     plt.axis('off')\n","    #     plt.savefig(f'./fea/{i}.png', bbox_inches='tight', dpi=300)\n","    #     plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"xLaraLEd0GUV"},"source":["## Show extracted feature from each layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cjJf1Sn9ypNm"},"outputs":[],"source":["from matplotlib.colors import CenteredNorm\n","def show_fea (length, y):\n","    plt.rcParams['savefig.dpi'] = 200\n","    plt.rcParams['figure.dpi'] = 200\n","    # length = len(y)\n","    # plt.figure(figsize=(8, length+2))\n","    # gs=gridspec.GridSpec(length, 8)\n","    # for i in range(length):\n","    length = 4\n","    plt.figure(figsize=(8, length+1))\n","    gs=gridspec.GridSpec(length, 8)\n","    depth_list = [2, 6, 14, 17] # UniFormer-S [3, 4, 8, 3]\n","\n","    for i in range(len(depth_list)):\n","        d_idx = depth_list[i]\n","        show_fea = y[d_idx]\n","        B, C, T, H, W = show_fea.shape\n","        show_fea = show_fea[0].view(C, T, H, W).permute(1, 2, 3, 0)\n","        for j in range(8):\n","            plt.subplot(gs[i,j])\n","            tmp_fea = -show_fea[j].mean(-1)\n","            print(tmp_fea.shape)\n","            plt.imshow(tmp_fea, norm=CenteredNorm(vcenter=tmp_fea.mean()))\n","\n","            plt.title(f'l_{d_idx}-t_{j}', fontsize=6)\n","            plt.axis('off')\n","\n","        # for j in range(8):\n","        #     tmp_fea = -show_fea[j].mean(-1)\n","        #     plt.imshow(tmp_fea, norm=CenteredNorm(vcenter=tmp_fea.mean()))\n","        #     # plt.imshow(-show_fea[j].mean(-1))\n","        #     plt.axis('off')\n","        #     plt.savefig(f'./fea/{i}_{j}.png', bbox_inches='tight', dpi=300)\n","        #     plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nnt_PFMNw9bB"},"source":["## Define train and test function"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":197,"status":"ok","timestamp":1697225323964,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"ZradzdHb6XMZ"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader\n","from torch import nn\n","import torchmetrics\n","from timeit import default_timer as timer\n","from tqdm.auto import tqdm\n","\n","def find_classes (directory):\n","    class_names = [item.name for item in scandir (directory) if item.is_dir()]\n","    if not class_names:\n","        raise FileNotFoundError (f\"Couldn't find any classes in {directory}\")\n","    class_to_idx = {class_name: i for i, class_name in enumerate (class_names)}\n","    return class_names, class_to_idx\n","\n","def print_train_time (start, end, device: torch.device = None):\n","  total_time = end - start\n","  print (f'Train time on {device}: {total_time:.3f} seconds')\n","\n","def train_step(model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer,\n","               device: str,\n","               val_dataloader = None,\n","               scheduler = None,\n","               scaler = None,\n","               checkpoint_path = None):\n","    # Put model in train mode\n","    model.train()\n","\n","    # Setup train loss and train accuracy values\n","    train_loss, train_acc = 0, 0\n","\n","    if not scaler:\n","        scaler = torch.cuda.amp.GradScaler()\n","    # Loop through data loader data batches\n","    print (f'Total batches: {len (dataloader)}')\n","    for batch, (X, y) in enumerate(dataloader):\n","        if batch % 20 == 0:\n","            print (f'Curent batch: {batch}')\n","        # Send data to target device\n","        X, y = X.to(device), y.to(device)\n","        # 1. Forward pass\n","        y_pred, y_fea = model(X)\n","\n","        # 2. Calculate  and accumulate loss\n","        loss = loss_fn(y_pred, y)\n","        train_loss += loss.item()\n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad(set_to_none=True)\n","\n","        # 4. Loss backward\n","        # loss.backward()\n","        scaler.scale(loss).backward()\n","        # 5. Optimizer step\n","        # optimizer.step()\n","        scaler.step(optimizer)\n","\n","        if scheduler:\n","            scheduler.step (loss)\n","            scheduler_state_dict = scheduler.state_dict()\n","        else:\n","            scheduler_state_dict = None\n","\n","        # Updates the scale for next iteration.\n","        scaler.update()\n","        # Calculate and accumulate accuracy metric across all batches\n","        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n","        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    train_loss = train_loss / len(dataloader)\n","    train_acc = train_acc / len(dataloader)\n","\n","    checkpoint_data = {\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","        'scheduler_state_dict':  scheduler.state_dict(),\n","        'scaler_state_dict': scaler.state_dict(),\n","    }\n","\n","    if checkpoint_path:\n","        torch.save (checkpoint_data, checkpoint_path)\n","        print (f'Saved the model to {checkpoint_path}')\n","\n","    if val_dataloader:\n","        val_loss, val_acc = test_step (model, val_dataloader, loss_fn, device)\n","        return train_loss, train_acc, val_loss, val_acc\n","\n","    return train_loss, train_acc\n","\n","def test_step(model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              loss_fn: torch.nn.Module,\n","              device: str):\n","    # Put model in eval mode\n","    model.eval()\n","\n","    # Setup test loss and test accuracy values\n","    test_loss, test_acc = 0, 0\n","\n","    # Turn on inference context manager\n","    with torch.inference_mode():\n","        # Loop through DataLoader batches\n","        for batch, (X, y) in enumerate(dataloader):\n","            if batch % 20 == 0:\n","                print (f'Curent batch: {batch}')\n","            # Send data to target device\n","            X, y = X.to(device), y.to(device)\n","\n","            # 1. Forward pass\n","            test_pred_logits, test_fea = model(X)\n","\n","            # 2. Calculate and accumulate loss\n","            loss = loss_fn(test_pred_logits, y)\n","            test_loss += loss.item()\n","\n","            # Calculate and accumulate accuracy\n","            test_pred_labels = test_pred_logits.argmax(dim=1)\n","            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n","\n","    # Adjust metrics to get average loss and accuracy per batch\n","    test_loss = test_loss / len(dataloader)\n","    test_acc = test_acc / len(dataloader)\n","\n","    return test_loss, test_acc\n","\n","def train(model: torch.nn.Module,\n","          train_dataloader: torch.utils.data.DataLoader,\n","          test_dataloader: torch.utils.data.DataLoader,\n","          loss_fn: torch.nn.Module,\n","          optimizer: torch.optim.Optimizer,\n","          device: str,\n","          epochs: int = 5,\n","          scheduler = None):\n","\n","    # 2. Create empty results dictionary\n","    results = {\"train_loss\": [],\n","        \"train_acc\": [],\n","        \"test_loss\": [],\n","        \"test_acc\": []\n","    }\n","\n","    # 3. Loop through training and testing steps for a number of epochs\n","    for epoch in tqdm(range(epochs)):\n","        train_loss, train_acc = train_step(model=model,\n","                                           dataloader=train_dataloader,\n","                                           loss_fn=loss_fn,\n","                                           optimizer=optimizer,\n","                                           device=device,\n","                                           scheduler=scheduler)\n","        if (epoch + 1) % 10 == 0:\n","            test_loss, test_acc = test_step(model=model,\n","                                            dataloader=test_dataloader,\n","                                            loss_fn=loss_fn,\n","                                            device=device)\n","\n","        # 4. Print out what's happening\n","        print(\n","            f\"Epoch: {epoch+1} | \"\n","            f\"train_loss: {train_loss:.4f} | \"\n","            f\"train_acc: {train_acc:.4f} | \"\n","        )\n","\n","        if (epoch + 1) % 10 == 0:\n","            print (\n","                f\"test_loss: {test_loss:.4f} | \"\n","                f\"test_acc: {test_acc:.4f}\"\n","            )\n","\n","        # 5. Update results dictionary\n","        results[\"train_loss\"].append(train_loss)\n","        results[\"train_acc\"].append(train_acc)\n","        if (epoch + 1) % 10 == 0:\n","            results[\"test_loss\"].append(test_loss)\n","            results[\"test_acc\"].append(test_acc)\n","            if test_acc >= 0.9:\n","                break\n","\n","    # 6. Return the filled results at the end of the epochs\n","    return results\n","\n","## Code to import this .py file to google colab if this file is saved in a folder in google drive\n","\n","# from google.colab import drive\n","# import sys\n","\n","# root = '/content/drive'\n","# drive.mount(root)\n","\n","# library_path = \"/content/drive/MyDrive/Colab Notebooks/PyTorch_showcase/lib\"  # Replace with the actual path\n","# sys.path.append(library_path)\n","\n","# import train_test_ML as tt\n","\n","# drive.flush_and_unmount()"]},{"cell_type":"markdown","metadata":{"id":"n_pJQtwtIxsV"},"source":["## Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2305,"status":"ok","timestamp":1697223979965,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"uxV7CEFNI7Qk","outputId":"a228d8af-3ea2-4392-f41c-5c4d13862dfd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import json\n","\n","# crop_size = 448\n","# scale_size = 512\n","# input_mean = [0.485, 0.456, 0.406]\n","# input_std = [0.229, 0.224, 0.225]\n","\n","# input_mean2 = [0.5, 0.5, 0.5]\n","# input_std2 = [0.5, 0.5, 0.5]\n","\n","# def normalize_0to1 (tensor: torch.tensor) -> torch.tensor:\n","#     \"\"\"transform torch.tensor image to range from 0 to 1\"\"\"\n","#     return tensor / 255.0\n","\n","# transform1 = torchvision.transforms.Compose([\n","#     t.Resize (scale_size),\n","#     t.CenterCrop(crop_size),\n","#     t.Lambda (normalize_0to1),\n","#     t.Normalize(input_mean, input_std),\n","# ])\n","\n","# transform2 = torchvision.transforms.Compose([\n","#     GroupScale(int(scale_size)),\n","#     GroupCenterCrop(crop_size),\n","#     Stack(),\n","# ])\n","\n","root = '/content/drive'\n","drive.mount(root)\n","\n","library_path = \"/content/drive/MyDrive/Colab Notebooks/PyTorch_showcase/lib\"  # Replace with the actual path\n","sys.path.append(library_path)\n","\n","class VideoFolderDataset (Dataset):\n","    def __init__ (self, targ_dir, transform_torch = None, n_segment = 11, sample_rate = 10, sample_method = 'dense', use_offsets = True):\n","        self.paths = list (Path (targ_dir).glob ('*/*.avi'))\n","        self.transform_torch = transform_torch\n","        self.n_segment = n_segment\n","        self.sample_rate = sample_rate\n","        self.sample_method = sample_method\n","        self.classes, self.class_to_idx = find_classes (targ_dir)\n","        self.use_offsets = use_offsets\n","        self.get_orig_frames = False\n","\n","    ''' Get VideoReader object and its theoretical total frames'''\n","    def get_vid (self, idx):\n","        video_path = self.paths[idx]\n","        return vr (str (video_path))\n","\n","    def get_realnum_total_frames (self, vid):\n","        vid_metadata = vid.get_metadata()['video']\n","        num_frames = vid_metadata['duration'][0] * vid_metadata['fps'][0]\n","        return floor (num_frames)\n","    ''' End                                                      '''\n","\n","    ''' Get Torch.tensor of video frames '''\n","    def get_stepframes_list (self, vid: torchvision.io.VideoReader, num_frames_subtraction = 0):\n","        frames = []\n","        num_frames = self.get_realnum_total_frames (vid)\n","        self.frame_idx = VideoFolderDataset.get_frame_idx (num_frames - num_frames_subtraction, self.n_segment, self.sample_rate, method = self.sample_method)\n","\n","        for num, frame in enumerate (vid):\n","            if num in self.frame_idx:\n","                frames.append (frame['data'])\n","            if len(frames) == self.n_segment:\n","                break\n","        return frames\n","\n","    def get_frames (self, idx, use_offsets = False):\n","        # num_frames = len (vid)\n","        if use_offsets == True:\n","            frames = list ()\n","            subtraction = 0\n","            while (len (frames) != self.n_segment):\n","                vid = self.get_vid (idx) # MUST put self.get_vid in while loop to create the iterator again in every loop because iterator can only be traversed once\n","                frames = self.get_stepframes_list (vid, subtraction)\n","                subtraction += 1\n","                if subtraction % 15 == 0:\n","                    print (f'{subtraction} or more frames are taken away from {self.paths[idx]}')\n","        else:\n","            vid = self.get_vid (idx)\n","            frames = []\n","            for frame in itertools.islice (vid.seek (1, keyframes_only = True), self.n_segment,):\n","                frames.append (frame['data'].to (dtype = torch.float))\n","                # if self.transform_torch:\n","                #     frames.append (self.transform_torch (frame['data'].to (dtype = torch.float)))\n","            if len (frames) != self.n_segment:\n","                frames = []\n","                for frame in itertools.islice (vid.seek (0, keyframes_only = True), self.n_segment):\n","                    frames.append (frame['data'].to (dtype = torch.float))\n","        return torch.stack (frames, 0)\n","    ''' End                                 '''\n","\n","    def get_label_idx (self, idx):\n","        class_name = self.paths[idx].parent.name\n","        return self.class_to_idx[class_name], class_name\n","\n","    def __len__ (self):\n","        return len (self.paths)\n","\n","    def __getitem__ (self, idx):\n","        label, _ = self.get_label_idx (idx)\n","\n","        frames = self.get_frames (idx, self.use_offsets)\n","        if self.get_orig_frames:\n","            frames_orig = frames.detach ().clone ()\n","        # vid = vid[frame_idx]\n","        if self.transform_torch:\n","            # vid = self.transform_torch(vid)\n","            frames = self.transform_torch (frames)\n","\n","        frames = frames.permute(1, 0, 2, 3) # transform from T, C, H, W to C, T, H, W\n","\n","        return (frames, label) if not self.get_orig_frames else (frames, label, frames_orig)\n","\n","    ''' Check if the self.get_frames () method return the same size (Tensor) for all Video since PyAV backend omit frames'''\n","    def check_num_frames (self):\n","        print (f'Total video: {self.__len__()}')\n","        for i, path in enumerate (self.paths):\n","            if (i+1)%50 == 0:\n","                print (f'Current index: {i}')\n","            videoreader = vr (str (path))\n","            frames = self.get_stepframes_list (videoreader)\n","            if len (frames) != self.n_segment:\n","                print (f'{path} does not have enough frames ({len(frames)} frames)')\n","                print (self.frame_idx)\n","\n","    def check_lost_frames (self):\n","        lost_frames = []\n","        for i in range (self.__len__()):\n","            if i % 50 == 0:\n","                print (f'Current index: {i}')\n","            vid = self.get_vid (i)\n","            real_total_frames = self.get_realnum_total_frames (vid)\n","            frames = []\n","            for frame in vid:\n","                frames.append (frame['data'])\n","            lost_frames.append (real_total_frames - len (frames))\n","\n","        print (f'Max frames loss: {max (lost_frames)}')\n","        print (f'Index of Max: {lost_frames.index (max (lost_frames))}')\n","        print (f'Min frames loss: {min (lost_frames)}')\n","    ''' End                                                                                                                 '''\n","\n","    def save_label_dict (self, save_dir):\n","        try:\n","            with open (save_dir, 'w') as f:\n","                json.dump (self.class_to_idx, f)\n","                print (f'Saved the label dictionary to {save_dir}')\n","        except IOError as e:\n","            print (f\"An error occured while trying to save label dictionary to {save_dir}: {e}\")\n","\n","    @staticmethod\n","    def get_frame_idx (num_frames, num_segments, dense_sample_rate = 6, method = 'dense'):\n","        sample_range = num_segments * dense_sample_rate\n","        if num_frames <= sample_range:\n","            method = None\n","            # print (f'Use method: None | Num frames: {num_frames} | Sampe range: {sample_range}')\n","\n","        if method == 'dense':\n","            sample_pos = max(1, 1 + num_frames - sample_range)\n","            t_stride = dense_sample_rate\n","            start_idx = 0 if sample_pos == 1 else sample_pos // 2\n","            # when position exceeds the total number of frames, it wraps around\n","            offsets = np.array([(idx * t_stride + start_idx) % num_frames for idx in range(num_segments)])\n","            # Discard the duplicated element and add another element\n","            # seen = set()\n","            # for i in range(len(offsets)):\n","            #     while offsets[i] in seen:\n","            #         offsets[i] = (offsets[i] + 1) % num_frames\n","            #     seen.add(offsets[i])\n","\n","            # offsets = np.array(offsets)\n","        else:\n","            if num_frames > num_segments:\n","                tick = num_frames / float(num_segments)\n","                offsets = np.array([int(tick / 2.0 + tick * x) for x in range(num_segments)])\n","            else:\n","                offsets = np.zeros((num_segments,))\n","        # print ('frame_idx: ',offsets)\n","        return offsets.astype (int)\n","\n","# g = torch.Generator ().manual_seed (42)\n","# dataset_dir = Path ('./video/UCF50')\n","# BATCH_SIZE = 8\n","# NUM_WORKERS = 2\n","\n","# video_data = VideoFolderDataset (dataset_dir, transform1)\n","# train_data, val_data, test_data = random_split (video_data, [0.6, 0.2, 0.2], generator = g)\n","# train_dataloader = DataLoader (train_data, batch_size = BATCH_SIZE, shuffle = True, num_workers  = NUM_WORKERS, pin_memory = True)\n","# val_dataloader = DataLoader (val_data, batch_size = BATCH_SIZE, shuffle = False, num_workers  = NUM_WORKERS, pin_memory = True)\n","# test_dataloader = DataLoader (test_data, batch_size = BATCH_SIZE, shuffle = False, num_workers  = NUM_WORKERS, pin_memory = True)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1697223979965,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"1jVotnQ3Ftm4","outputId":"6e899b33-67f1-47a4-e9bd-58eaf774d193"},"outputs":[{"name":"stdout","output_type":"stream","text":["[ 4 12 20 28 36 44 52 60 68 76 84 92]\n"]}],"source":["print (VideoFolderDataset.get_frame_idx (97, 12, method = None))"]},{"cell_type":"markdown","metadata":{"id":"C2nKF2mxF7F_"},"source":["## Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"gD1vE-zq402M"},"source":["### Method II"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ViDH612_SoY7"},"outputs":[],"source":["def method_II ():\n","\n","        model.head = nn.Identity ()\n","        linear_model = LinearModel (n_inputs, n_classes).to (device)\n","        model.to (device)\n","\n","        scaler = torch.cuda.amp.GradScaler()\n","        loss_fn = nn.CrossEntropyLoss ()\n","        optimizer = optim.AdamW (params = linear_model.parameters (), lr = 0.01)\n","        scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_dataloader), epochs=4, div_factor=10.0)\n","        # Loop through DataLoader batches\n","        train_loss, train_acc = 0, 0\n","        print (f'Total batches: {len (train_dataloader)}')\n","        start = timer ()\n","        for batch, (X, y) in enumerate(train_dataloader):\n","            # Send data to target device\n","            X, y = X.to(device), y.to(device)\n","            model.eval ()\n","            with torch.no_grad():\n","                X_vector, y_fea = model(X)\n","\n","            if batch % 20 == 0:\n","                print (f'Curent batch: {batch}')\n","\n","            model.train ()\n","            # 1. Forward pass\n","            y_pred = linear_model (X_vector)\n","\n","            # 2. Calculate  and accumulate loss\n","            loss = loss_fn(y_pred, y)\n","            train_loss += loss.item()\n","\n","            # 3. Optimizer zero grad\n","            optimizer.zero_grad(set_to_none=True)\n","\n","            # 4. Loss backward\n","            # loss.backward()\n","            scaler.scale(loss).backward()\n","            # 5. Optimizer step\n","            # optimizer.step()\n","            scaler.step(optimizer)\n","\n","            if scheduler:\n","                scheduler.step (loss)\n","\n","            # Updates the scale for next iteration.\n","            scaler.update()\n","            # Calculate and accumulate accuracy metric across all batches\n","            y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n","            train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n","\n","        end = timer ()\n","        print_train_time (start, end, device)\n","        # Adjust metrics to get average loss and accuracy per batch\n","        train_loss = train_loss / len(train_dataloader)\n","        train_acc = train_acc / len(train_dataloader)\n","\n","        print(\n","            f\"train_loss: {train_loss:.4f} | \"\n","            f\"train_acc: {train_acc:.4f} | \"\n","        )\n","        return linear_model"]},{"cell_type":"markdown","metadata":{"id":"UxSU_oCX4_wi"},"source":["### Create transfer learning model"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":172,"status":"ok","timestamp":1697225333085,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"qtmEdeCsohso"},"outputs":[],"source":["from google.colab import drive\n","import sys\n","from pathlib import Path\n","from torch.optim import lr_scheduler\n","import itertools\n","from math import floor\n","\n","from ray import tune\n","from ray.train import Checkpoint\n","from ray.tune.schedulers import ASHAScheduler\n","\n","from functools import partial\n","\n","class LinearModel (nn.Module):\n","    def __init__ (self, n_inputs, n_outputs, dropout_rate = 0.2):\n","        super().__init__ ()\n","        self.n_inputs = n_inputs\n","        self.n_classes = n_outputs\n","\n","        self.linear_layer_complex = nn.Sequential (nn.Linear (self.n_inputs, 256),\n","                                nn.ReLU (),\n","                                nn.Dropout (dropout_rate),\n","                                nn.Linear (256, self.n_classes),\n","                                nn.LogSoftmax (dim = 1) )\n","\n","        linear_layer =  nn.Linear (self.n_inputs, self.n_classes)\n","        linear_layer.weight = nn.init.zeros_ (linear_layer.weight)\n","        linear_layer.bias = nn.init.zeros_ (linear_layer.bias)\n","        self.linear_layer_simple = linear_layer\n","\n","    def forward (self, x):\n","        return self.linear_layer_complex (x)\n","\n","## Method I: change the last layer of the orig model\n","def create_transfer_learning_model (model, video_data, device, dropout_rate):\n","\n","    n_classes = len (video_data.classes)\n","    n_inputs = model.head.in_features\n","\n","    # Freeze the parameters of every layers\n","    for param in model.parameters ():\n","        param.requires_grad = False\n","\n","    print (f'Classifier layer before update: {model.get_classifier ()}')\n","    # Change the last layer of the model (default: require_grad = True)\n","    model.head = nn.Sequential (model.head,\n","                                nn.Dropout (0.2),\n","                                LinearModel (model.head.out_features, n_classes).linear_layer_simple,)\n","\n","    for param in model.head.parameters ():\n","        param.requires_grad=True\n","\n","    params_to_update = {}\n","    for name, param in model.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update[name] = param\n","\n","    print (f'Params to update: {params_to_update}')\n","    print (f'Classifier layer after update: {model.get_classifier ()}')\n","\n","    model.to (device)\n","    return model\n","\n","    ## Method II: extract feature vector then use it as input for the new model\n","    # model_1 = method_II ()"]},{"cell_type":"markdown","metadata":{"id":"n6CqB-vidUUr"},"source":["#Ray Tuning"]},{"cell_type":"markdown","metadata":{"id":"_U2H7r121We5"},"source":["## train function"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":202,"status":"ok","timestamp":1697225981048,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"Y9LdfNDH1SNm"},"outputs":[],"source":["def train_UCF50 (model, device, epochs, video_data, checkpoint_path = None, config = None, ray = True):\n","    if not config:\n","        ray = None\n","        config = {}\n","        config['dropout_rate'] = 0.2\n","\n","    BATCH_SIZE = 8\n","    NUM_WORKERS = 2\n","    g = torch.Generator ().manual_seed (42)\n","\n","    train_data, val_data, test_data = random_split (video_data, [0.6, 0.1, 0.3], generator = g)\n","    train_dataloader = DataLoader (train_data, batch_size = BATCH_SIZE, shuffle = True, num_workers  = NUM_WORKERS, pin_memory = True)\n","    val_dataloader = DataLoader (val_data, batch_size = BATCH_SIZE, shuffle = False, num_workers  = NUM_WORKERS, pin_memory = True)\n","    test_dataloader = DataLoader (test_data, batch_size = BATCH_SIZE, shuffle = False, num_workers  = NUM_WORKERS, pin_memory = True)\n","    print (f'Total Video: {video_data.__len__()}')\n","    print (f'Video in Train Set: {len (train_data)}')\n","    print (f'Video in Val Set: {len (val_data)}')\n","    print (f'Video in Test Set: {len (test_data)}')\n","\n","    loss_fn = nn.CrossEntropyLoss ()\n","    optimizer = optim.AdamW (params = model.parameters (), lr = 0.01)\n","    scheduler = lr_scheduler.OneCycleLR(optimizer, max_lr=config['lr'], steps_per_epoch=len(train_dataloader), epochs=4, div_factor=10.0)\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    if checkpoint_path is not None:\n","        if Path (checkpoint_path).exists ():\n","            print ('Take state_dict from previous checkpoint')\n","            checkpoint = torch.load(checkpoint_path)\n","            model.load_state_dict(checkpoint['model_state_dict'])\n","            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","            # scaler.load_state_dict (checkpoint['scaler_state_dict'])\n","\n","            print ('Classifier after load check point:')\n","            print (model.head.state_dict())\n","\n","    for epoch in tqdm(range(epochs)):\n","        train_loss, train_acc, val_loss, val_acc = train_step (model, train_dataloader, loss_fn, optimizer, device, val_dataloader,\n","                                                                scheduler, checkpoint_path = checkpoint_path)\n","        if ray:\n","            # Ray tune\n","            checkpoints = Checkpoint.from_directory(checkpoint_path)\n","            train.report(\n","                {\"loss\": val_loss, \"accuracy\": val_acc},\n","                checkpoint = checkpoints,)\n","\n","        if (epoch + 1) % 10 == 0:\n","            test_loss, test_acc = test_step(model=model,\n","                                            dataloader=test_dataloader,\n","                                            loss_fn=loss_fn,\n","                                            device=device)\n","\n","        print (f'Current learning rate: {scheduler.get_last_lr()}')\n","\n","        print ('Most updated classifier:')\n","        print (model.head.state_dict())\n","        # 4. Print out what's happening\n","        print(\n","            f\"Epoch: {epoch+1} | \"\n","            f\"train_loss: {train_loss:.4f} | \"\n","            f\"train_acc: {train_acc:.4f} | \"\n","        )\n","\n","        if (epoch + 1) % 10 == 0:\n","            print (\n","                f\"test_loss: {test_loss:.4f} | \"\n","                f\"test_acc: {test_acc:.4f}\"\n","            )\n","            if test_acc >= 0.9:\n","                break\n","        if train_acc >= 0.9:\n","            break"]},{"cell_type":"markdown","metadata":{"id":"dh0Bo6TKRRjZ"},"source":["## Main without Ray Tune"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["44429be577f947bf80ce79e07819d7d1","9889bdd4fbbf4109a7dc816392c9a44a","2e8c4d9a5b2b421d85ca29625eb92012","a098403d71a542438f7438a48df6ed4b","0bb0970e6c2345309a2db6662383c375","301c2f7586914b4183975a1845bd5860","e5c77568c6b44a189c16823f2f922052","0b3730191ed54625ab601f6d4eae3864","b7b64fae58034c3085a2f5a06dfea2bf","3a915ca6815648cf88251ef789edfa2f","85690e6808dd45f186a3eeedcfb887e7"]},"executionInfo":{"elapsed":69286,"status":"ok","timestamp":1697225408486,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"D_5rwOU7RVmq","outputId":"16b2739b-1ba7-4456-941b-8fdea58ea26d"},"outputs":[],"source":["def main ():\n","    crop_size = 448\n","    scale_size = 512\n","    input_mean = [0.485, 0.456, 0.406]\n","    input_std = [0.229, 0.224, 0.225]\n","\n","    input_mean2 = [0.5, 0.5, 0.5]\n","    input_std2 = [0.5, 0.5, 0.5]\n","\n","    def normalize_0to1 (tensor: torch.tensor) -> torch.tensor:\n","        \"\"\"transform torch.tensor image to range from 0 to 1\"\"\"\n","        return tensor / 255.0\n","\n","    transform1 = torchvision.transforms.Compose([\n","        t.Resize (scale_size),\n","        t.CenterCrop(crop_size),\n","        t.Lambda (normalize_0to1),\n","        t.Normalize(input_mean, input_std),\n","    ])\n","\n","    config = {\n","        'dropout_rate': 0.2,\n","        'batch_size': 10,\n","        'lr': 0.01,\n","    }\n","    model_path = path.join('pretrained_model', 'uniformer_small_k400_16x8.pth')\n","    state_dict = torch.load (model_path, map_location='cpu')\n","    model = uniformer_small ()\n","    model.load_state_dict (state_dict)\n","\n","    model_structure = [module for module in model.modules() if not isinstance(module, torch.nn.Sequential)]\n","    # print (f'\\tMODEL STRUCTURE:\\n\\n{model_structure}') # Uncomment to print out the model's structure\n","\n","    device = 'cuda' if torch.cuda.is_available else 'cpu'\n","    # device = 'cpu'\n","    print (f'Device using: {device}')\n","\n","    dataset_dir = Path ('/content/UniFormer/video_classification/video/UCF50')\n","\n","    video_data = VideoFolderDataset (dataset_dir, transform1)\n","    model = create_transfer_learning_model (model, video_data, device, config['dropout_rate'])\n","\n","    EPOCHS = 1\n","    checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/USF_VideoUnderstanding/TransferModel/SmallUniformK400_SimpleLinearModel_checkpoint.pt\"\n","    label_index_path = \"/content/drive/MyDrive/Colab Notebooks/USF_VideoUnderstanding/TransferModel/label_dict.json\"\n","    video_data.save_label_dict (label_index_path)\n","\n","    train_UCF50 (model, device, EPOCHS, video_data, checkpoint_path, config, ray = False)\n","\n","    # Delete model to free memory because checkpoint_path is provided\n","    import gc\n","    del model\n","    gc.collect()\n","\n","main ()"]},{"cell_type":"markdown","metadata":{"id":"j0xMPHpwRK0M"},"source":["## Main for Ray Tune"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4F_t71TbdWJG","outputId":"02d8819e-bbfc-4832-9f22-3d2f782bd4f5"},"outputs":[],"source":["from ray.tune.search.optuna import OptunaSearch\n","\n","def main(num_samples=3, max_num_epochs=10):\n","    crop_size = 448\n","    scale_size = 512\n","    input_mean = [0.485, 0.456, 0.406]\n","    input_std = [0.229, 0.224, 0.225]\n","\n","    input_mean2 = [0.5, 0.5, 0.5]\n","    input_std2 = [0.5, 0.5, 0.5]\n","\n","    def normalize_0to1 (tensor: torch.tensor) -> torch.tensor:\n","        \"\"\"transform torch.tensor image to range from 0 to 1\"\"\"\n","        return tensor / 255.0\n","\n","    transform1 = torchvision.transforms.Compose([\n","        t.Resize (scale_size),\n","        t.CenterCrop(crop_size),\n","        t.Lambda (normalize_0to1),\n","        t.Normalize(input_mean, input_std),\n","    ])\n","    config = {\n","        'dropout_rate': tune.choice ([0.1, 0.2, 0.3]),\n","        'batch_size': tune.choice ([8, 10]),\n","        'lr': tune.loguniform(1e-2, 1e-1),\n","    }\n","\n","    tune_scheduler = ASHAScheduler(\n","        max_t=max_num_epochs,\n","        grace_period=1,\n","        reduction_factor=2)\n","\n","    model_path = path.join('pretrained_model', 'uniformer_small_k400_16x8.pth')\n","    state_dict = torch.load (model_path, map_location='cpu')\n","    model = uniformer_small ()\n","    model.load_state_dict (state_dict)\n","\n","    model_structure = [module for module in model.modules() if not isinstance(module, torch.nn.Sequential)]\n","    # print (f'\\tMODEL STRUCTURE:\\n\\n{model_structure}') # Uncomment to print out the model's structure\n","\n","    device = 'cuda' if torch.cuda.is_available else 'cpu'\n","    # device = 'cpu'\n","    print (f'Device using: {device}')\n","\n","    dataset_dir = Path ('/content/UniFormer/video_classification/video/UCF50')\n","\n","    video_data = VideoFolderDataset (dataset_dir, transform1)\n","    model = create_transfer_learning_model (model, video_data, device, config['dropout_rate'])\n","\n","    EPOCHS = 5\n","    checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/USF_VideoUnderstanding/TransferModel/SmallUniformK400_SimpleLinearModel_checkpoint.pt\"\n","    label_index_path = \"/content/drive/MyDrive/Colab Notebooks/USF_VideoUnderstanding/TransferModel/label_dict.json\"\n","    video_data.save_label_dict (label_index_path)\n","\n","    algo = OptunaSearch()\n","    tuner = tune.Tuner (\n","        tune.with_resources(tune.with_parameters(partial(train_UCF50, model, device, EPOCHS, video_data, checkpoint_path)),\n","                                                 resources={\"cpu\": 2, \"gpu\": 1}),\n","        tune_config=tune.TuneConfig(\n","                    metric=\"loss\",\n","                    mode=\"min\",\n","                    scheduler=tune_scheduler,\n","                    num_samples=num_samples,\n","                    search_alg=algo),\n","        param_space=config\n","    )\n","\n","    results = tuner.fit()\n","\n","    best_result = results.get_best_result(\"loss\", \"min\")\n","\n","    print(\"Best trial config: {}\".format(best_result.config))\n","    print(\"Best trial final validation loss: {}\".format(\n","        best_result.metrics[\"loss\"]))\n","    print(\"Best trial final validation accuracy: {}\".format(\n","        best_result.metrics[\"accuracy\"]))\n","    # print (next (iter (train_dataloader)))\n","    drive.flush_and_unmount()\n","\n","main(num_samples=2, max_num_epochs=2)"]},{"cell_type":"markdown","metadata":{"id":"JZQV-5HpJIvK"},"source":["# Load model from checkpoint to evaluate"]},{"cell_type":"markdown","metadata":{"id":"nShxCJo9AFSH"},"source":["## Load from checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2124,"status":"ok","timestamp":1695933614272,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"jlQ_Q48VfwV3","outputId":"f6476c1e-c6e8-46c9-b2f9-4bb3d6425732"},"outputs":[],"source":["from timm.layers.linear import Linear\n","import copy\n","\n","root = '/content/drive'\n","drive.mount(root)\n","\n","device = 'cuda'\n","\n","model_path = path.join('pretrained_model', 'uniformer_small_k400_16x8.pth')\n","state_dict = torch.load (model_path, map_location='cpu')\n","model = uniformer_small ()\n","model.load_state_dict (state_dict)\n","\n","\n","checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/USF_VideoUnderstanding/TransferModel/SmallUniformK400_SimpleLinearModel_checkpoint.pt\"\n","checkpoint_state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))['model_state_dict']\n","checkpoint_model = copy.deepcopy (model).to (device)\n","n_classes = len (video_data.classes)\n","n_inputs = model.head.in_features\n","checkpoint_model.head = nn.Sequential (model.head,\n","                                       nn.Dropout (0.2),\n","                                       nn.Linear (400, 50)).to (device)\n","print (checkpoint_model.head.state_dict())\n","checkpoint_model.load_state_dict (checkpoint_state_dict)\n","print ('Afer load from checkpoint...')\n","print (checkpoint_model.head.state_dict())\n","\n","model_structure = [module for module in model.modules() if not isinstance(module, torch.nn.Sequential)]"]},{"cell_type":"markdown","metadata":{"id":"I8vRLGNLAQRg"},"source":["## Evaluate on Test Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2987,"status":"ok","timestamp":1695935530046,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"bhWhnQxeAS6G","outputId":"422f2b8e-dacc-4fe9-9e42-5fe74afc6362"},"outputs":[],"source":["# Check params\n","for p1, p2 in zip(checkpoint_model.to(device).parameters(), model.to(device).parameters()):\n","    if not (torch.equal(p1, p2)):\n","        print (p1.data.shape)\n","\n","# test_loss, test_acc = test_step (checkpoint_model, test_dataloader, nn.CrossEntropyLoss (), device)\n","# print (f'test loss: {test_loss} | test acc: {test_acc}')"]},{"cell_type":"markdown","metadata":{"id":"62mT_xqlAb0W"},"source":["## Show video and predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O36Ig14iQFGX"},"outputs":[],"source":["video_data.get_orig_frames = True\n","_, test_data = random_split (video_data, [0.7, 0.3], generator = g)\n","test_dataloader = DataLoader (test_data, batch_size = 1, shuffle = False)\n","test_data_iterator = iter (test_dataloader)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"elapsed":7431,"status":"ok","timestamp":1695933389561,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"YJtYgLARJSSu","outputId":"126c80d0-1fac-4ba9-9b32-f2240e19a4bc"},"outputs":[],"source":["def vidshow (*vid: torch.Tensor, true_label, predict_model = None, classes = None, device = 'cuda'):\n","    '''\n","    Take a [T, C, H, W] Tensor to plot.\n","    Tensor.dtype == 0-255 torch.float32\n","    '''\n","    plt.rcParams['savefig.dpi'] = 200\n","    plt.rcParams['figure.dpi'] = 200\n","    plt.figure(figsize=(16, 4))\n","\n","    if len (vid) == 2:\n","        vid_tensor, vid_augmented = vid[0].squeeze(), vid[1]\n","    else:\n","        vid_tensor = vid[0].squeeze()\n","\n","    col = int (vid_tensor.shape[0] / 2)\n","    gs = gridspec.GridSpec(2, col, wspace=0.0, hspace=0.0)\n","\n","    if predict_model:\n","        model.eval()\n","        with torch.inference_mode():\n","            vid_augmented = vid_augmented.to(device)\n","            test_pred_logits, test_fea = predict_model(vid_augmented)\n","            test_pred_labels = test_pred_logits.argmax(dim=1)\n","\n","        predict_label = classes[test_pred_labels[0]]\n","\n","    for i in range(2):\n","        for j in range(col):\n","            plt.subplot(gs[i, j])\n","            temp = vid_tensor[j+col*i, :] / 255.\n","            plt.imshow(temp.permute (1, 2, 0))\n","            plt.axis('off')\n","\n","    plt.suptitle(f'Predict: {predict_label}, True: {true_label}', x=0.5, y=1.05, ha='center', fontsize=14)\n","    plt.show ()\n","\n","# index = random.randrange (len (video_data))\n","# index = 4520\n","# print (f'Video index using: {index}')\n","# video = video_data.get_frames (index, use_offsets = True)\n","# _, label = video_data.get_label_idx (index)\n","\n","X_augmented, y, X_orig = next (test_data_iterator)\n","label = video_data.classes[y]\n","vidshow (X_orig, X_augmented, true_label = label, predict_model = checkpoint_model, classes = video_data.classes)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"elapsed":1517,"status":"error","timestamp":1695402379207,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"M7ryBzZ9_Dp7","outputId":"cb41ff46-089c-483d-d847-4d15b1e14c97"},"outputs":[],"source":["def check_num_frames (vid_paths, n_segment = 8):\n","    for path in vid_paths:\n","        vid_reader = vr (str (path))\n","        frames = []\n","        for frame in itertools.islice (vid_reader.seek (0, keyframes_only = True), n_segment):\n","            frames.append (frame['data'].to (dtype = torch.float))\n","\n","        if len (frames) != n_segment:\n","            print (path)\n","\n","vid_paths = list (Path (dataset_dir).glob ('*/*.avi'))\n","check_num_frames (vid_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":524,"status":"ok","timestamp":1695793586208,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"Fk7fayPZLFMf","outputId":"93899ca9-4c3c-4597-9341-a5d549d7f21b"},"outputs":[],"source":["next (iter (test_data))"]},{"cell_type":"markdown","metadata":{"id":"aP4UqQyXFJZy"},"source":["## Cuda Inspection"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1697225967419,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"atPqx5uBndOe"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"elapsed":361,"status":"error","timestamp":1697223049484,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"285B9y9UnrvC","outputId":"077e361e-5e87-4484-f9f7-9f8bb1dd952f"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-68e6f215253e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["import gc\n","del model\n","gc.collect()"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":186,"status":"ok","timestamp":1697225969617,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"SxARc76AZxud","outputId":"1bec1843-c2e2-4b4d-a502-1a8f3c2395b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      | 531807 KiB |  11032 MiB |   7054 GiB |   7054 GiB |\n","|       from large pool | 452608 KiB |  10991 MiB |   7051 GiB |   7050 GiB |\n","|       from small pool |  79199 KiB |     89 MiB |      3 GiB |      3 GiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         | 531807 KiB |  11032 MiB |   7054 GiB |   7054 GiB |\n","|       from large pool | 452608 KiB |  10991 MiB |   7051 GiB |   7050 GiB |\n","|       from small pool |  79199 KiB |     89 MiB |      3 GiB |      3 GiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      | 524916 KiB |  11025 MiB |   7054 GiB |   7053 GiB |\n","|       from large pool | 445952 KiB |  10984 MiB |   7050 GiB |   7050 GiB |\n","|       from small pool |  78964 KiB |     88 MiB |      3 GiB |      3 GiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |   1022 MiB |  13364 MiB |  25520 MiB |  24498 MiB |\n","|       from large pool |    936 MiB |  13278 MiB |  25384 MiB |  24448 MiB |\n","|       from small pool |     86 MiB |     90 MiB |    136 MiB |     50 MiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory | 514721 KiB |   5854 MiB |   3666 GiB |   3666 GiB |\n","|       from large pool | 505856 KiB |   5854 MiB |   3661 GiB |   3661 GiB |\n","|       from small pool |   8865 KiB |     15 MiB |      4 GiB |      4 GiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |    1986    |    2319    |   38480    |   36494    |\n","|       from large pool |     206    |     267    |   25562    |   25356    |\n","|       from small pool |    1780    |    2079    |   12918    |   11138    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |    1986    |    2319    |   38480    |   36494    |\n","|       from large pool |     206    |     267    |   25562    |   25356    |\n","|       from small pool |    1780    |    2079    |   12918    |   11138    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |      56    |      65    |     100    |      44    |\n","|       from large pool |      13    |      23    |      32    |      19    |\n","|       from small pool |      43    |      45    |      68    |      25    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |      14    |      31    |   18072    |   18058    |\n","|       from large pool |       3    |      27    |   12304    |   12301    |\n","|       from small pool |      11    |      18    |    5768    |    5757    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n"]}],"source":["print(torch.cuda.memory_summary(device=None, abbreviated=False))"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":392,"status":"ok","timestamp":1697225156683,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"BFU-vW5y9wTu","outputId":"7e0308da-c271-4b2d-f77c-ce5be01c83bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Fri Oct 13 19:25:56 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   77C    P0    32W /  70W |   1125MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":408,"status":"ok","timestamp":1695401443099,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"q61sGUOvxAZc","outputId":"3897108d-5560-468c-c528-df683de8f568"},"outputs":[{"name":"stdout","output_type":"stream","text":["<torchvision.io.video_reader.VideoReader object at 0x790174b09f00>\n"]},{"data":{"text/plain":["275"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["frm_idx = [11,  35,  59,  83, 107, 131, 154, 178, 202, 226, 250, 274]\n","frames = []\n","i = 0\n","print (vid)\n","stream = \"video\"\n","video_path = video_data.paths[4520]\n","vid = vr (str(video_path))\n","for frame in vid:\n","    frames.append (frame['data'])\n","len (frames)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":514},"executionInfo":{"elapsed":160,"status":"error","timestamp":1695399893336,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"uIOmoXRwRmSa","outputId":"e487ce52-1351-4e82-a224-538c82df1436"},"outputs":[],"source":["from torchvision.io import read_video\n","video_1 = read_video (video_path)\n","video_frames = []\n","for frame in itertools.islice(vid.seek(0), 0, 12*12, 12):\n","    video_frames.append (frame['data'])\n","\n","print (len (video_frames))\n","\n","# videos = torch.stack(video_frames, 0)\n","# print (video_1[0].dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1645,"status":"ok","timestamp":1697153896915,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"gCSuHpnVCFZj","outputId":"926f99bc-04e5-455b-be6b-bf9bc173da66"},"outputs":[],"source":["%cd UniFormer/video_classification\n","!python setup.py build develop"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":204,"status":"ok","timestamp":1697224741112,"user":{"displayName":"Vu Duc Anh Trinh","userId":"10091321529737837155"},"user_tz":240},"id":"2rH2VaXZTA7o","outputId":"d610aa5e-c5f1-4f0e-d868-3bcbb3279449"},"outputs":[{"data":{"text/plain":["(0, 0, 0, 0, 0)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["def test (val = None):\n","    train_acc = 0\n","    train_loss = 0\n","    if val:\n","        val_acc = 0\n","        val_loss = 0\n","    return train_acc, train_loss if not val else train_acc, train_loss, val_acc, val_loss\n","\n","test ([1,2])"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNLXOdPQDObZH+htu1FHc3Y","collapsed_sections":["K93akE52tBCh","LinSCUSnv_gn","t7_06w-zyYAY","pFg7WztzEZ02","OvqRMxzCVI0r","xLaraLEd0GUV","n_pJQtwtIxsV","gD1vE-zq402M","dh0Bo6TKRRjZ","nShxCJo9AFSH","I8vRLGNLAQRg","62mT_xqlAb0W"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b3730191ed54625ab601f6d4eae3864":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bb0970e6c2345309a2db6662383c375":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e8c4d9a5b2b421d85ca29625eb92012":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b3730191ed54625ab601f6d4eae3864","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7b64fae58034c3085a2f5a06dfea2bf","value":0}},"301c2f7586914b4183975a1845bd5860":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a915ca6815648cf88251ef789edfa2f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44429be577f947bf80ce79e07819d7d1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9889bdd4fbbf4109a7dc816392c9a44a","IPY_MODEL_2e8c4d9a5b2b421d85ca29625eb92012","IPY_MODEL_a098403d71a542438f7438a48df6ed4b"],"layout":"IPY_MODEL_0bb0970e6c2345309a2db6662383c375"}},"85690e6808dd45f186a3eeedcfb887e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9889bdd4fbbf4109a7dc816392c9a44a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_301c2f7586914b4183975a1845bd5860","placeholder":"","style":"IPY_MODEL_e5c77568c6b44a189c16823f2f922052","value":"  0%"}},"a098403d71a542438f7438a48df6ed4b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a915ca6815648cf88251ef789edfa2f","placeholder":"","style":"IPY_MODEL_85690e6808dd45f186a3eeedcfb887e7","value":" 0/1 [01:07&lt;?, ?it/s]"}},"b7b64fae58034c3085a2f5a06dfea2bf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5c77568c6b44a189c16823f2f922052":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
